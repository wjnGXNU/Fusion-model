import torch
import torch.nn as nn
import numpy as np



class MultiHeadAttention(nn.Module):

    def __init__(self, n_head, d_model, d_k, d_v, dropout=0.1):
        super().__init__()


class SDPAttention(nn.Module):

    def __init__(self, temperature, attn_dropout=0.1):
        super().__init__()
        
